from llama_cpp import Llama

# Kh·ªüi t·∫°o m√¥ h√¨nh
print("ƒêang t·∫£i m√¥ h√¨nh...")
llm = Llama(
    model_path="./models/mistral-7b-openorca.Q4_0.gguf",
    n_gpu_layers=0,  # Th·ª≠ =0 n·∫øu kh√¥ng c√≥ GPU
    n_ctx=4096,
    verbose=False  # T·∫Øt log chi ti·∫øt n·∫øu kh√¥ng c·∫ßn
)
print("M√¥ h√¨nh ƒë√£ s·∫µn s√†ng!\n")

# L·ªãch s·ª≠ h·ªôi tho·∫°i
conversation_history = """<|im_start|>system
You are a helpful chatbot.
<|im_end|>"""

# V√≤ng l·∫∑p chatbot
print("=" * 50)
print("ü§ñ CHATBOT ƒê√É S·∫¥N S√ÄNG!")
print("G√µ 'quit', 'exit' ho·∫∑c 'q' ƒë·ªÉ tho√°t")
print("=" * 50)
print()

while True:
    # Nh·∫≠n input t·ª´ ng∆∞·ªùi d√πng
    user_input = input("B·∫°n: ").strip()
    
    # Ki·ªÉm tra l·ªánh tho√°t
    if user_input.lower() in ['quit', 'exit', 'q', 'tho√°t']:
        print("\nüëã T·∫°m bi·ªát!")
        break
    
    if not user_input:
        continue
    
    # Th√™m c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠
    conversation_history += f"\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant"
    
    # T·∫°o completion
    print("\nü§ñ Bot: ", end='', flush=True)
    output = llm.create_completion(
        conversation_history,
        max_tokens=500,
        stop=["<|im_end|>"],
        stream=True
    )
    
    # In response t·ª´ng token
    assistant_response = ""
    for token in output:
        text = token["choices"][0]["text"]
        print(text, end='', flush=True)
        assistant_response += text
    
    print("\n")  # Xu·ªëng d√≤ng
    
    # Th√™m ph·∫£n h·ªìi v√†o l·ªãch s·ª≠
    conversation_history += assistant_response + "<|im_end|>"